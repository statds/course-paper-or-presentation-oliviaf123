
\documentclass[12pt]{article}

%% preamble: Keep it clean; only include those you need
\usepackage{amsmath}
\usepackage[margin = 1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{natbib}

% for space filling
\usepackage{lipsum}
% highlighting hyper links
\usepackage[colorlinks=true, citecolor=blue]{hyperref}


%% meta data

\title{Attempting to predict Heart Disease using a classical decision tree machine learning method }
\author{Olivia Frillici\\
  Department of Statistics\\
  University of Connecticut
}

\begin{document}
\maketitle
\begin{abstract}
  Heart disease has been the leading cause of death globally for 20 years \citep{khan2020global}. The extraordinary availability of patient data in combination with the potential for great impact derived from successful statistical models serves as motivation for data analysis and model creation. The goal of this paper is to create a classical decision tree model using the largest heart disease data set available to predict heart disease. A classical decision tree with all variables in the data set had an error rate of .4967, a second model where individuals with a serum cholesterol level of 0 mg/dl were removed had an error rate of .5201, and a third model where the variable chest pain type was removed provided an error rate of .4924. The results provided by this paper reconfirm that classical decision trees are not the best model making method for heart disease. 
    \end{abstract}
    
    
    \section{Introduction}
    \label{sec:intro}
    
    To continue living a human’s heart needs to pump blood. Blood carries oxygen to our vital organs and also can take away the trash that cells produce as they do their daily tasks. Heart disease makes it hard for the human body to function properly. By researching and developing tools in which medical providers can input patient data and get outputs pertaining to if a patient has heart disease, medical providers can prescribe lifestyle changes to mitigate and decrease risk of significant cardiac events. 
1.72\% of the world’s population, which is about 126 million individuals, are affected by heart disease \citep{khan2020global}. Additionally 9 million deaths were caused by heart disease globally which makes it the leading cause of death and it has retained this status for the last 20 years\citep{khan2020global}. Heart disease is not only extraordinarily prevalent it is also a quite expensive disease to deal with. In 2010 it cost 863 billion us dollars on a global scale \citep{khan2020global}. 
The prevalence of heart disease, in conjunction with the increased availability of anonymous patient data has led to many people attempting to create models which based on a set of input variables can predict heart disease. An example of such work is \citet{5643666} where they compared performances of 3 different models including a decision tree, their tree model ended up having a .21 error rate. 
In this paper a multitude of classical decision trees were grown with the largest heart disease data set available to explore the error rate and if certain data and model manipulation could improve the error rate.


    % roadmap
%The rest of the paper is organized as follows.
%The data will be presented in Section~\ref{sec:data}.
%The methods are described in Section~\ref{sec:meth}.
%The results are reported in Section~\ref{sec:resu}.
%A discussion concludes in Section~\ref{sec:disc}.


\section{Data}
\label{sec:data}

The data is acquired from kaggle and is the “largest heart disease data set available”. The creator of the data set combined a total of 5 datasets which had 11 overlapping variables. There are 918 observations. The variables in the data set are as follows: Age,measured in years, sex, male or female, ChestPainType, TA for typical angina, ATA for atypical angina, NAP for non-anginal pain, and ASY for asymptomatic, RestingBP, resting blood pressure measured in mm Hg, Cholesterol, serum cholesterol measured in mg/dL, FastingBS, fasting blood sugar, a binary variable where 1 equals FastingBS greater than 120 mg/dl and 0 otherwise, RestingECG, a binary variable for resting electrocardiogram results where Normal is Normal and ST represents having an ST wave abnormality, MaxHR, maximum heart rate achieved, Exercise angina, a binary yes or no variable, Oldpeak, a numeric value of ST wave depression, ST\_Slope, a categorical variable that places the slope of the peak exercise ST segment of a heartbeat wave into Up for upsloping, flat for flat, and down for downsloping, and lastly HeartDisease, the variable of interest, a binary variable where 1 represents an individual with heart disease and 0 for an individual that does not have heart disease. As expected with any dataset some data cleaning and variable visualization was required for developing and fitting the model. \par
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{Sex_HeartDisease.png}
  \caption{A comparative bar chart showing the distribution of gender and heart disease.}
  \label{fig:heartdisease_and_sex}
\end{figure}
Figure~\ref*{fig:heartdisease_and_sex} shows that within this data set more females did not have heart disease than females who did. The opposite is evidently true for males, more males had heart disease in the data set than males who did not have heart disease. 
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{Age_HeartDisease.png}
  \caption{A comparative boxplot showing median age of individuals in the data set with and without heart disease.}
  \label{fig:age_heartdisease}
\end{figure}
In figure~\ref*{fig:age_heartdisease} the Age and Heart Disease comparative boxplot shows that the median age for individuals who had heart disease is greater than the individuals who did not. 
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{Cholesterol_Heart Disease.png}
  \caption{A comparative boxplot showing serum cholesterol level mg/dL for individuals in the data set with and without heart disease.}
  \label{fig:cholesterol_heartdisease}
\end{figure}
In figure~\ref*{fig:cholesterol_heartdisease} the Cholesterol and Heart Disease comparative boxplots the medians for individuals who did and did not have heart disease seem to be very similar. Within the cholesterol data column there are over 100 observations with a value of 0, so when constructing the comparative boxplot the 0 values were removed so as not to extraordinarily skew the results.  
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{RBP_HeartDisease.png}
  \caption{A comparative boxplot showing resting blood pressure in mmHg of individuals in the data set with and without heart disease.}
  \label{fig:rbp_heartdisease}
\end{figure}
In figure~\ref*{rbp_heartdisease} the resting blood pressure and heart disease comparative boxplots keep the same theme as figure~\ref*{fig:cholesterol_heartdisease}. The median resting blood pressure for individuals with heart disease is slightly higher than for those without heart disease. However the difference is so mild it appears insignificant. 
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{FastingBloodSugar_HeartDisease.png}
  \caption{A proportional bar chart that shows frequency of individuals who have a fasting blood sugar value of greater than 120 mg/dl or less than within the groups of individuals who have heart disease and who don't.}
  \label{fig:FBS_heartdisease}
\end{figure}
Figure~\ref*{fig:FBS_heartdisease} the fasting blood sugar and heart disease proportional bar chart shows that a very high proportion of the individuals that do not have heart disease also have a fasting blood sugar value of less than 120 mg/dl. The bar chart also shows that a large proportion of individuals who have heart disease also have a fasting blood sugar of less than 120 mg/dl. Finally this barchart shows that while individuals who have a fasting blood sugar that is greater than 120 mg/dl are in the minority there is a higher proportion of them who have heart disease than not. 
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{RestingECG_HeartDisease.png}
  \caption{A porportional bar chart that shows the frequency of individuals who have a resting electrocardiogram test result of normal or an elctrocardiogram result depicting an ST wave abnormality within the groups of individuals who have or do not have heart disease.}
  \label{fig:ECG_heartdisease}
\end{figure}
Figure~\ref*{fig:ECG_heartdisease} shows that a majority of individuals both with and without heart disease have normal resting ECG outputs. This graph shows a similar trend to figure~\ref*{fig:FBS_heartdisease} in that while having an ST wave abnormality is clearly in the minority a larger proportion of individuals affected by the abnormality also have heart disease.
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{MaxHR_HeartDisease.png}
  \caption{A comparative boxplot showing maximum heart rate achieved in beats per minute for individuals in the data set with and without heart disease.}
  \label{fig:maxHR_heartdisease}
\end{figure}
Figure~\ref*{fig:maxHR_heartdisease}shows that the median max heart rate of individuals without heart disease is higher than that for individuals with heart disease. 
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{ExerciseInducedAngina_HeartDisease.png}
  \caption{A proportional bar chart that shows the frequency of individuals who have exercise induced angina within the groups of individuals who have or don't have heart disease.}
  \label{fig:EIA_heartdisease}
\end{figure}
Figure\ref*{fig:EIA_heartdisease}shows that a high proportion of individuals who do not have heart disease also do not have exercise induced angina. Additionally, a high proportion of individuals who do have heart disease also have exercise induced angina. 
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{STSlope_HeartDisease.png}
  \caption{A proportional barchart that shows the frequency of individuals who have an upward sloping, downward sloping, or flat ST segment of their peak exercise heart beat wave within the groups of individuals who have or don't have deart disease.}
  \label{fig:STslope_heartdisease}
\end{figure}
Finally figure~\ref*{fig:STslope_heartdisease} shows that a high proportion of individuals who do not have heart disease have an upward slope on the ST segment of their peak exercise heart beat wave. Conversely a high proportion of individuals who have heart disease have a flat slope on the ST segment of their peak exercise heart beat wave. Additionally while in both the group with and without heart disease a downward sloping ST segment of the peak exercise heart beat wave was a very small minority of individuals, there was a higher proportion in individuals who had heart disease compared to those that did not. \par 




\section{Methods}
\label{sec:meth}


The aim for this paper is to create a classical decision tree machine learning model that given variable inputs can predict whether an individual has heart disease or not.
Three classical decision trees were created. One included all of the variables and all individuals, one included all variables however the data has been modified to void any individuals that had a cholesterol level of 0 mg/dL. The final model created had been modified to include all variables except for chest pain type and ST slope. \par
Before any classical decision trees were made the data was split into 2 equal sized sets. One served as the training set and the other served as the validation set. A seed number of 4321 was used for the sample function in r so that results could be reproducible. \par
After the classical decision tree the decision regarding trimming the tree was made a by plotting the cross validated error against the complexity parameter. Trees considered for the final model had points below the horizontal line on the cp graph. \par
To assess the model fit each model was validated with the validation set created at the beginning and the error rate was calculated by the sum of false negative and false positives over the total number of individuals in the set. \par


\section{Results}
\label{sec:resu}
As stated in the methods section, error rate was used to determine model fit and success.
The first model created, the one with all variables and all observations included produced an  error rate of .496732. This means that 49.67\% of the validation observations were misclassified by the model. 
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{cpGraph_AllVars.png}
  \caption{Cross validationerror versus the complexity parameter graph for the model developed with all variables and all individuals included.}
  \label{fig:cpGraph_AllVars}
\end{figure}
After producing this model and investigating figure~\ref*{cpGraph_AllVars},the cross validation error versus the complexity parameter graph, it was determined that no pruning was needed. No pruning was needed because as shown in figure~\ref*{cpGraph_AllVars} all of the different trees fall below the horizontal line plotted on the graph. Large error rate motivated data modification and reassessment. \par
The next model that was made trimmed the number of observations by if an individual had a serum cholesterol level of 0 mg/dL. 
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{cpGraph_NoCholesterol.png}
  \caption{Cross validation error versus the complexity parameter graph for the model developed with no individuals that had a serum cholesterol level of 0 mg/dL.}
  \label{fig:cpGraph_NoCholesterol}
\end{figure}
As shown in figure~\ref*{fig:cpGraph_NoCholesterol} it is seen that a tree with 2 terminal nodes,4 terminal nodes, 6 terminal nodes, 7 terminal nodes, and 11 terminal nodes all fit within the cirterion of having a relative error that is below the horizontal line plotted on the graph. Generally a smaller tree is reckognized as better because the computer has to do less work. Therefore the two sincere contenders for this model are the tree with 2 terminal nodes and one split or the tree with 4 terminal nodes and 3 splits. The model was trimmed to have 3 splits because 1 split felt a little too simplistic. After pruning the tree the error rate was found to be .5201. This means that 52.01\% of the validation data was misclassified. This ended up being the worst performing model with the greatest error rate. \par
The final model created was modified to not include chest pain type because it is a variable that has 4 different classifications and usually variables that are numeric or have binary classification work much better. Additionally ST slope was also removed because another variable, Oldpeak, in the data set provides a numeric measurement for this. Also the ST Slope variable, similar to the chest pain type variable, has 3 different classifications instead of 2.
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{cpGraph_NoSTSandCPT.png}
  \caption{Cross validation error versus the complexity parameter graph for the model developed with no the ST Slope and Chest Pain Type variables removed.}
  \label{fig:cpGraph_NoSTSandCPT}
\end{figure} 
This tree also showed qualities which would make it a good candidate for pruning. As seen in figure~\ref*{fig:cpGraph_NoSTSandCPT} it is shown that both a tree with 3 terminal nodes and 2 splits or 6 terminal nodes and 5 splits would be optimal. However, after attempting pruning in a bevvy of ways my program would seemingly not do any pruning. The error rate for this tree, with 6 terminal nodes and 5 splits, ended up being .4924. This means 49.24\% of the data in the validation set was misclassified. This model was surprisingly the one with the lowest error rate. \par
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{DTree_noSTSandCPT.png}
  \caption{Best performing classical decision tree, with an error rate of .4924, out of the three. This tree was constructed without ST Slope and Chest Pain Type variables. }
  \label{fig:DTree_noSTSandCPT}
\end{figure} 
The final and best decision tree out of the three models constructed in this paer is shown in figure~\ref*{fig:DTree_noSTSandCPT}. Exercise Angina, Cholesterol, and Old Peak variables are deemed the most significant at determining heart disease in individuals in this model. \par
\section{Discussion}
\label{sec:disc}

In conclusion, the error rates of all three models hovered around 50\%. This is not a good result. Since the first error rate was so large in the first model with all of the variables I was led to try different models to see if they would improve the error rate. While each error rate is not identical I would argue that the differences between them are insignificant. I wanted to create a model without the cholesterol values of 0 mg/dL because if the values reported are truly the serum levels of cholesterol it would be impossible if not extremely unhealthy for these to be 0 mg/dL. Originally I thought the creator of the data frame might have switched individuals who maybe didn’t have cholesterol levels to report from NA to 0 just so the usability and number of NA values was 0. Additionally after getting 2 bad results I decided to reasses and reflect, go through my notes and think about what could possibly make this model better. I found that these models work best with variables that are either numeric where a cut off point can be determined or strictly binary variables. This led me to take out the chest pain type because it had 4 buckets instead of 2. I also took out the ST slope variable because it seemed redundant and it also had 3 buckets of classification instead of 2. This model technically ended up having the smallest error rate but it still was around 50\%. \par 
Limitations in this paper mainly stem from the data source itself. Since there are over 100 individuals with claimed blood serum cholesterol levels of 0 mg/dL when this is not medically possible I think that data point may have been misrepresented. Additionally even though there were a significant amount of variables provided I think the addition of more variables like weight, BMI, exercise frequency, or a quantitative measure of healthy lifestyle might make better less error prone models. I also think the method chosen to create the models was limiting. Since all of the models have a 50\% error rate I can conclude and reconfirm that classical decision trees are not be the best machine learning model for this data. At the time of writing the proposal for this project and then when developing the rough draft and doing the actual coding and model creation for this project I did not have the knowledge of many other methods of machine learning for binary variables at the time. As the semester as continued and my knowledge from other classes has expanded the breadth of statisitical idease that I know, it would be incredibly beneficial to delve into models such as logistic regression, linear discriminant analysis, quadratic discriminant analysis, and deep learning models such as feed foward networks. Finally while this data set was large and quite comprehensive with more time, money, and resources more individuals and variables could be added to the data set. \par
Finally and most importantly, this study as it stands in its current state proves something very important: computers won\textsc{\char13}t be overtaking a doctor\textsc{\char13}s role anytime soon. The grueling years of training and studying still prevails over a tiny microchip in a motherboard. The results of this study however should not discourage the motivation of trying to come up with statistical models for medical outcomes. This paper should merely mark a moment in time. In the future as statistical knowledge increases, as more patient data is released, and as computers get better, faster, and smarter, someday a computer may help a doctor in diagnosis of heart disease.


\bibliography{refs}
\bibliographystyle{ama}

\end{document}